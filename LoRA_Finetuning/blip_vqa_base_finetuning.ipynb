{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForQuestionAnswering, Trainer, TrainingArguments\nimport torch\nfrom accelerate import Accelerator\nfrom sklearn.model_selection import train_test_split\nfrom peft import LoraConfig, get_peft_model\nfrom transformers.data.data_collator import default_data_collator\n\n# Initialize Accelerator for efficient multi-GPU training\naccelerator = Accelerator()\n\n# Load BLIP-1 VQA processor and model with fast image processing\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\", use_fast=True)\nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n\n# Define dataset paths (adjust these to your Kaggle environment)\nVQA_DATASET_PATH = '/kaggle/input/vqa-dataset/VQA_Dataset.csv'\nABO_METADATA_PATH = '/kaggle/input/abo-small/metadata/images.csv'\nABO_IMAGE_BASE_PATH = '/kaggle/input/abo-small/small'\n\n# Load VQA dataset\nvqa_df = pd.read_csv(VQA_DATASET_PATH)\nprint(f\"Loaded VQA dataset with {len(vqa_df)} entries\")\n\n# Load ABO metadata and merge to get image paths\nabo_metadata = pd.read_csv(ABO_METADATA_PATH)\nvqa_df = pd.merge(vqa_df, abo_metadata[['image_id', 'path']], on='image_id', how='left')\n\n# Handle missing values and ensure answers are strings\nvqa_df['answer'] = vqa_df['answer'].fillna('unknown').astype(str)\n\n# Split data into train and test sets\ntrain_df, test_df = train_test_split(vqa_df, test_size=0.2, random_state=51)\nprint(f\"Training set size: {len(train_df)}, Test set size: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:11:23.522140Z","iopub.execute_input":"2025-05-13T13:11:23.522747Z","iopub.status.idle":"2025-05-13T13:12:03.336313Z","shell.execute_reply.started":"2025-05-13T13:11:23.522719Z","shell.execute_reply":"2025-05-13T13:12:03.335569Z"}},"outputs":[{"name":"stderr","text":"2025-05-13 13:11:39.138652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747141899.357673      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747141899.422797      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6125dfff1b664316b30f26aeea3be08a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2656db033824edcbf4cff8f35d28ffa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f3a5f61f1894c20a31c4461d163cda4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a1340526aa94629bfbe98609eb3f918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4accea0baf60481e9b8db00df6a92822"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bde2c4bbf674819b8e0c2dc42648ac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38eb7dfb5fab4b07a6b7bebca7b0eeff"}},"metadata":{}},{"name":"stdout","text":"Loaded VQA dataset with 64406 entries\nTraining set size: 51524, Test set size: 12882\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Define a custom VQA Dataset class\nclass VQADataset(torch.utils.data.Dataset):\n    def __init__(self, df, processor, image_base_path):\n        self.df = df\n        self.processor = processor\n        self.image_base_path = image_base_path\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_base_path, row['path'])\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading image {image_path}: {e}\")\n            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n        \n        question = row['question']\n        answer = row['answer']\n\n        # Process image and question with attention mask\n        encoding = self.processor(\n            images=image,\n            text=question,\n            padding=\"max_length\",\n            max_length=128,  # Fixed length for input_ids and attention_mask\n            truncation=True,\n            return_tensors=\"pt\",\n            return_attention_mask=True  # Explicitly include attention_mask\n        )\n        \n        # Tokenize answer as labels with fixed length\n        labels = self.processor.tokenizer(\n            answer,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=32,  # Fixed length for labels\n            return_tensors=\"pt\"\n        )[\"input_ids\"]\n\n        # Remove batch dimension from tensors\n        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n        encoding[\"labels\"] = labels.squeeze(0)\n\n        return encoding\n\n# Create training dataset\ntrain_dataset = VQADataset(train_df, processor, ABO_IMAGE_BASE_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:12:06.270024Z","iopub.execute_input":"2025-05-13T13:12:06.270817Z","iopub.status.idle":"2025-05-13T13:12:06.278492Z","shell.execute_reply.started":"2025-05-13T13:12:06.270782Z","shell.execute_reply":"2025-05-13T13:12:06.277736Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Define LoRA configuration\nlora_config = LoraConfig(\n    r=16,  # Rank of low-rank matrices\n    lora_alpha=32,  # Scaling factor\n    target_modules=[\"query\", \"value\"],  # Target attention layers in BLIP-1\n    lora_dropout=0.1,  # Dropout for regularization\n    bias=\"none\"  # No bias adaptation\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\nprint(\"LoRA applied to the model\")\n\n# Prepare model with Accelerator\nmodel = accelerator.prepare(model)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    run_name=\"blip_vqa_lora_finetune\",  # Unique run name\n    num_train_epochs=3,\n    per_device_train_batch_size=4,  # Reduced for memory stability\n    gradient_accumulation_steps=4,  # Simulate larger batch size (effective batch size = 16)\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    fp16=True,  # Mixed precision for efficiency\n    remove_unused_columns=False,  # Keep all dataset columns\n    report_to=\"none\"  # Disable W&B and other logging integrations\n)\n\n# Create Trainer instance with default data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=default_data_collator,  # Handle tensor stacking\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:12:10.593602Z","iopub.execute_input":"2025-05-13T13:12:10.593889Z","iopub.status.idle":"2025-05-13T13:12:12.411831Z","shell.execute_reply.started":"2025-05-13T13:12:10.593867Z","shell.execute_reply":"2025-05-13T13:12:12.411155Z"}},"outputs":[{"name":"stdout","text":"LoRA applied to the model\n","output_type":"stream"},{"name":"stderr","text":"No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Check GPU memory usage before training\nif torch.cuda.is_available():\n    print(\"GPU Memory Usage Before Training:\")\n    print(torch.cuda.memory_summary())\n\n# Start fine-tuning with LoRA\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.save_model(\"./fine_tuned_blip_vqa_lora\")\nprint(\"Model saved to './fine_tuned_blip_vqa_lora'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:12:17.836808Z","iopub.execute_input":"2025-05-13T13:12:17.837448Z","iopub.status.idle":"2025-05-13T17:34:16.240759Z","shell.execute_reply.started":"2025-05-13T13:12:17.837417Z","shell.execute_reply":"2025-05-13T17:34:16.239806Z"}},"outputs":[{"name":"stdout","text":"GPU Memory Usage Before Training:\n|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |   1479 MiB |   1479 MiB |   1479 MiB |      0 B   |\n|       from large pool |   1468 MiB |   1468 MiB |   1468 MiB |      0 B   |\n|       from small pool |     10 MiB |     10 MiB |     10 MiB |      0 B   |\n|---------------------------------------------------------------------------|\n| Active memory         |   1479 MiB |   1479 MiB |   1479 MiB |      0 B   |\n|       from large pool |   1468 MiB |   1468 MiB |   1468 MiB |      0 B   |\n|       from small pool |     10 MiB |     10 MiB |     10 MiB |      0 B   |\n|---------------------------------------------------------------------------|\n| Requested memory      |   1476 MiB |   1476 MiB |   1476 MiB |      0 B   |\n|       from large pool |   1465 MiB |   1465 MiB |   1465 MiB |      0 B   |\n|       from small pool |     10 MiB |     10 MiB |     10 MiB |      0 B   |\n|---------------------------------------------------------------------------|\n| GPU reserved memory   |   1622 MiB |   1622 MiB |   1622 MiB |      0 B   |\n|       from large pool |   1610 MiB |   1610 MiB |   1610 MiB |      0 B   |\n|       from small pool |     12 MiB |     12 MiB |     12 MiB |      0 B   |\n|---------------------------------------------------------------------------|\n| Non-releasable memory | 145971 KiB | 148413 KiB |    958 MiB |    815 MiB |\n|       from large pool | 144896 KiB | 147200 KiB |    946 MiB |    804 MiB |\n|       from small pool |   1075 KiB |   2045 KiB |     11 MiB |     10 MiB |\n|---------------------------------------------------------------------------|\n| Allocations           |     982    |     982    |     982    |       0    |\n|       from large pool |     296    |     296    |     296    |       0    |\n|       from small pool |     686    |     686    |     686    |       0    |\n|---------------------------------------------------------------------------|\n| Active allocs         |     982    |     982    |     982    |       0    |\n|       from large pool |     296    |     296    |     296    |       0    |\n|       from small pool |     686    |     686    |     686    |       0    |\n|---------------------------------------------------------------------------|\n| GPU reserved segments |      76    |      76    |      76    |       0    |\n|       from large pool |      70    |      70    |      70    |       0    |\n|       from small pool |       6    |       6    |       6    |       0    |\n|---------------------------------------------------------------------------|\n| Non-releasable allocs |      70    |      70    |      73    |       3    |\n|       from large pool |      64    |      64    |      67    |       3    |\n|       from small pool |       6    |       6    |       6    |       0    |\n|---------------------------------------------------------------------------|\n| Oversize allocations  |       0    |       0    |       0    |       0    |\n|---------------------------------------------------------------------------|\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\n|===========================================================================|\n\n","output_type":"stream"},{"name":"stderr","text":"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\nWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4830' max='4830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4830/4830 4:21:52, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>10.252200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>10.053800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>9.879100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>9.689300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>9.528200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>9.412500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>9.250600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>9.158200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>9.039600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>8.940900</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>8.856400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>8.753600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>8.714800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>8.639400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>8.608000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>8.555900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>8.534000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>8.495700</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>8.476700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>8.450200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>8.415600</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>8.419400</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>8.393500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>8.375600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>8.365600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>8.318500</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>8.298700</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>8.280400</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>8.275200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>8.267800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>8.255800</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>8.256700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>8.242700</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>8.252900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>8.227100</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>8.240500</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>8.231900</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>8.231900</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>8.242400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>8.241000</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>8.224000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>8.216100</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>8.227600</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>8.214500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>8.217400</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>8.206500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>8.215800</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>8.223300</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>8.203900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>8.211700</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>8.202500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>8.204200</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>8.203600</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>8.206100</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>8.197100</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>8.199600</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>8.199100</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>8.196100</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>8.196900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>8.201300</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>8.192000</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>8.190400</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>8.191100</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>8.203500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>8.196800</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>8.190800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>8.182000</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>8.188400</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>8.196700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>8.191100</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>8.187300</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>8.191900</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>8.193300</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>8.188100</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>8.177800</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>8.188800</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>8.175100</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>8.183600</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>8.189200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>8.183800</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>8.171100</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>8.187800</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>8.170200</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>8.184800</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>8.181000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>8.169300</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>8.190800</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>8.164100</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>8.179800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>8.179600</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>8.183000</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>8.157300</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>8.172000</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>8.176000</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>8.177500</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>8.189600</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>8.178200</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>8.172600</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>8.175100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>8.188500</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>8.185600</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>8.178000</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>8.174000</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>8.178800</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>8.186500</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>8.162500</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>8.167200</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>8.173900</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>8.176700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>8.179000</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>8.178500</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>8.165600</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>8.169500</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>8.164000</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>8.166300</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>8.166900</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>8.172200</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>8.170600</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>8.161400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>8.159700</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>8.168700</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>8.179900</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>8.144800</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>8.162900</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>8.167700</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>8.159100</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>8.166800</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>8.157200</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>8.162100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>8.165000</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>8.147200</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>8.168000</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>8.171400</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>8.159300</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>8.159200</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>8.161900</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>8.161100</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>8.174600</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>8.165600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>8.169100</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>8.164300</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>8.158100</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>8.161900</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>8.152300</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>8.154500</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>8.162700</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>8.171400</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>8.165600</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>8.154400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>8.174200</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>8.160100</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>8.168000</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>8.155000</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>8.169600</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>8.162800</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>8.162500</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>8.152100</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>8.160300</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>8.174300</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>8.165300</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>8.164500</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>7.543000</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>8.157700</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>8.157100</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>8.165400</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>8.145600</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>8.153600</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>8.158600</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>8.169300</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>8.163900</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>8.148600</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>8.154300</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>8.147500</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>8.140100</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>8.160000</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>8.164300</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>8.147900</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>8.151000</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>8.149400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>8.165400</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>8.154700</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>8.140400</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>8.159300</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>8.158500</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>8.150400</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>8.160500</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>8.147300</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>8.149900</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>8.160600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>8.141800</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>8.143400</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>8.155100</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>8.148100</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>8.148900</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>8.156900</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>8.148300</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>8.163800</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>8.160300</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>8.161600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>8.160200</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>8.125900</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>8.142600</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>8.155100</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>8.146100</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>8.145000</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>8.166600</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>8.154300</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>8.158200</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>8.163400</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>8.147600</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>8.151600</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>8.154000</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>8.146100</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>8.164100</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>8.160500</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>8.153300</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>8.160600</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>8.158900</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>8.145800</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>8.147200</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>8.157800</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>8.151100</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>8.147600</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>8.157900</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>8.163800</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>8.160600</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>8.149600</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>8.157900</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>8.149500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>8.164200</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>8.149500</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>8.147500</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>8.158100</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>8.142400</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>8.137900</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>8.135700</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>8.151200</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>8.140000</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>8.134000</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>8.132300</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>8.159000</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>8.151800</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>8.153300</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>8.154400</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>8.153600</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>8.160300</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>8.156200</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>8.156500</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>8.144200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>8.162600</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>8.146800</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>8.160600</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>8.144900</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>8.156800</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>8.157700</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>8.150600</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>8.156100</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>8.148600</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>8.156700</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>8.146400</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>8.140900</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>8.149100</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>8.154900</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>8.148100</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>8.144200</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>8.140800</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>8.145300</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>8.135300</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>8.150600</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>8.135000</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>8.148400</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>8.143400</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>8.147500</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>8.151400</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>8.149700</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>8.140700</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>8.141500</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>8.131600</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>8.157700</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>8.145800</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>8.154900</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>8.148700</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>8.141300</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>8.154600</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>8.142100</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>8.149100</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>8.147700</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>8.146900</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>8.139600</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>8.128700</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>8.138500</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>8.157300</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>8.144700</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>8.151700</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>8.148600</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>8.156300</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>8.138000</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>8.150600</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>8.143900</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>8.144700</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>8.147600</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>8.154400</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>8.152600</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>8.142800</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>8.142100</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>8.147500</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>8.140600</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>8.143800</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>8.155000</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>8.152100</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>8.150500</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>8.135600</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>8.144900</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>8.144800</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>8.151100</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>8.154400</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>8.138600</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>8.146600</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>8.147800</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>8.146500</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>8.131600</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>8.128000</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>7.539700</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>8.147800</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>8.143500</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>8.150800</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>8.140000</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>8.137000</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>8.151100</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>8.147600</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>8.155300</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>8.139800</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>8.160600</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>8.149400</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>8.134700</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>8.134300</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>8.139600</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>8.136600</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>8.160400</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>8.134700</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>8.142400</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>8.146300</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>8.133600</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>8.141800</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>8.148700</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>8.136400</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>8.151300</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>8.138300</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>8.139800</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>8.140900</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>8.149100</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>8.140800</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>8.142600</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>8.134400</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>8.144700</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>8.152000</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>8.162900</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>8.143700</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>8.136400</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>8.150100</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>8.141300</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>8.151200</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>8.153600</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>8.134700</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>8.138500</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>8.134800</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>8.153900</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>8.132600</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>8.126900</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>8.141700</td>\n    </tr>\n    <tr>\n      <td>3710</td>\n      <td>8.152900</td>\n    </tr>\n    <tr>\n      <td>3720</td>\n      <td>8.148400</td>\n    </tr>\n    <tr>\n      <td>3730</td>\n      <td>8.141600</td>\n    </tr>\n    <tr>\n      <td>3740</td>\n      <td>8.152100</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>8.146900</td>\n    </tr>\n    <tr>\n      <td>3760</td>\n      <td>8.148400</td>\n    </tr>\n    <tr>\n      <td>3770</td>\n      <td>8.139000</td>\n    </tr>\n    <tr>\n      <td>3780</td>\n      <td>8.134500</td>\n    </tr>\n    <tr>\n      <td>3790</td>\n      <td>8.134000</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>8.155600</td>\n    </tr>\n    <tr>\n      <td>3810</td>\n      <td>8.144700</td>\n    </tr>\n    <tr>\n      <td>3820</td>\n      <td>8.143200</td>\n    </tr>\n    <tr>\n      <td>3830</td>\n      <td>8.141100</td>\n    </tr>\n    <tr>\n      <td>3840</td>\n      <td>8.144100</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>8.141700</td>\n    </tr>\n    <tr>\n      <td>3860</td>\n      <td>8.153800</td>\n    </tr>\n    <tr>\n      <td>3870</td>\n      <td>8.146700</td>\n    </tr>\n    <tr>\n      <td>3880</td>\n      <td>8.144200</td>\n    </tr>\n    <tr>\n      <td>3890</td>\n      <td>8.142900</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>8.139000</td>\n    </tr>\n    <tr>\n      <td>3910</td>\n      <td>8.131100</td>\n    </tr>\n    <tr>\n      <td>3920</td>\n      <td>8.131500</td>\n    </tr>\n    <tr>\n      <td>3930</td>\n      <td>8.129300</td>\n    </tr>\n    <tr>\n      <td>3940</td>\n      <td>8.132800</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>8.144100</td>\n    </tr>\n    <tr>\n      <td>3960</td>\n      <td>8.142100</td>\n    </tr>\n    <tr>\n      <td>3970</td>\n      <td>8.152100</td>\n    </tr>\n    <tr>\n      <td>3980</td>\n      <td>8.137200</td>\n    </tr>\n    <tr>\n      <td>3990</td>\n      <td>8.131700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>8.153700</td>\n    </tr>\n    <tr>\n      <td>4010</td>\n      <td>8.148500</td>\n    </tr>\n    <tr>\n      <td>4020</td>\n      <td>8.137900</td>\n    </tr>\n    <tr>\n      <td>4030</td>\n      <td>8.137700</td>\n    </tr>\n    <tr>\n      <td>4040</td>\n      <td>8.151300</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>8.144900</td>\n    </tr>\n    <tr>\n      <td>4060</td>\n      <td>8.136000</td>\n    </tr>\n    <tr>\n      <td>4070</td>\n      <td>8.146300</td>\n    </tr>\n    <tr>\n      <td>4080</td>\n      <td>8.166200</td>\n    </tr>\n    <tr>\n      <td>4090</td>\n      <td>8.135100</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>8.144400</td>\n    </tr>\n    <tr>\n      <td>4110</td>\n      <td>8.142000</td>\n    </tr>\n    <tr>\n      <td>4120</td>\n      <td>8.132200</td>\n    </tr>\n    <tr>\n      <td>4130</td>\n      <td>8.134800</td>\n    </tr>\n    <tr>\n      <td>4140</td>\n      <td>8.137100</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>8.138200</td>\n    </tr>\n    <tr>\n      <td>4160</td>\n      <td>8.146200</td>\n    </tr>\n    <tr>\n      <td>4170</td>\n      <td>8.130600</td>\n    </tr>\n    <tr>\n      <td>4180</td>\n      <td>8.135400</td>\n    </tr>\n    <tr>\n      <td>4190</td>\n      <td>8.138200</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>8.141300</td>\n    </tr>\n    <tr>\n      <td>4210</td>\n      <td>8.135600</td>\n    </tr>\n    <tr>\n      <td>4220</td>\n      <td>8.140500</td>\n    </tr>\n    <tr>\n      <td>4230</td>\n      <td>8.136700</td>\n    </tr>\n    <tr>\n      <td>4240</td>\n      <td>8.138100</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>8.140700</td>\n    </tr>\n    <tr>\n      <td>4260</td>\n      <td>8.154400</td>\n    </tr>\n    <tr>\n      <td>4270</td>\n      <td>8.147300</td>\n    </tr>\n    <tr>\n      <td>4280</td>\n      <td>8.148300</td>\n    </tr>\n    <tr>\n      <td>4290</td>\n      <td>8.143600</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>8.142200</td>\n    </tr>\n    <tr>\n      <td>4310</td>\n      <td>8.143600</td>\n    </tr>\n    <tr>\n      <td>4320</td>\n      <td>8.124200</td>\n    </tr>\n    <tr>\n      <td>4330</td>\n      <td>8.133000</td>\n    </tr>\n    <tr>\n      <td>4340</td>\n      <td>8.126200</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>8.132000</td>\n    </tr>\n    <tr>\n      <td>4360</td>\n      <td>8.145100</td>\n    </tr>\n    <tr>\n      <td>4370</td>\n      <td>8.142900</td>\n    </tr>\n    <tr>\n      <td>4380</td>\n      <td>8.146600</td>\n    </tr>\n    <tr>\n      <td>4390</td>\n      <td>8.147800</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>8.114700</td>\n    </tr>\n    <tr>\n      <td>4410</td>\n      <td>8.134200</td>\n    </tr>\n    <tr>\n      <td>4420</td>\n      <td>8.142300</td>\n    </tr>\n    <tr>\n      <td>4430</td>\n      <td>8.136600</td>\n    </tr>\n    <tr>\n      <td>4440</td>\n      <td>8.145000</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>8.126700</td>\n    </tr>\n    <tr>\n      <td>4460</td>\n      <td>8.130600</td>\n    </tr>\n    <tr>\n      <td>4470</td>\n      <td>8.142000</td>\n    </tr>\n    <tr>\n      <td>4480</td>\n      <td>8.129800</td>\n    </tr>\n    <tr>\n      <td>4490</td>\n      <td>8.130800</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>8.136400</td>\n    </tr>\n    <tr>\n      <td>4510</td>\n      <td>8.143900</td>\n    </tr>\n    <tr>\n      <td>4520</td>\n      <td>8.151900</td>\n    </tr>\n    <tr>\n      <td>4530</td>\n      <td>8.145800</td>\n    </tr>\n    <tr>\n      <td>4540</td>\n      <td>8.140400</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>8.151600</td>\n    </tr>\n    <tr>\n      <td>4560</td>\n      <td>8.149500</td>\n    </tr>\n    <tr>\n      <td>4570</td>\n      <td>8.148800</td>\n    </tr>\n    <tr>\n      <td>4580</td>\n      <td>8.146400</td>\n    </tr>\n    <tr>\n      <td>4590</td>\n      <td>8.139500</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>8.123700</td>\n    </tr>\n    <tr>\n      <td>4610</td>\n      <td>8.139100</td>\n    </tr>\n    <tr>\n      <td>4620</td>\n      <td>8.143600</td>\n    </tr>\n    <tr>\n      <td>4630</td>\n      <td>8.136300</td>\n    </tr>\n    <tr>\n      <td>4640</td>\n      <td>8.137700</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>8.131200</td>\n    </tr>\n    <tr>\n      <td>4660</td>\n      <td>8.140600</td>\n    </tr>\n    <tr>\n      <td>4670</td>\n      <td>8.151100</td>\n    </tr>\n    <tr>\n      <td>4680</td>\n      <td>8.135400</td>\n    </tr>\n    <tr>\n      <td>4690</td>\n      <td>8.143100</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>8.140200</td>\n    </tr>\n    <tr>\n      <td>4710</td>\n      <td>8.134400</td>\n    </tr>\n    <tr>\n      <td>4720</td>\n      <td>8.125100</td>\n    </tr>\n    <tr>\n      <td>4730</td>\n      <td>8.137500</td>\n    </tr>\n    <tr>\n      <td>4740</td>\n      <td>8.138000</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>8.126200</td>\n    </tr>\n    <tr>\n      <td>4760</td>\n      <td>8.126200</td>\n    </tr>\n    <tr>\n      <td>4770</td>\n      <td>8.150300</td>\n    </tr>\n    <tr>\n      <td>4780</td>\n      <td>8.134300</td>\n    </tr>\n    <tr>\n      <td>4790</td>\n      <td>8.136400</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>8.127100</td>\n    </tr>\n    <tr>\n      <td>4810</td>\n      <td>8.134600</td>\n    </tr>\n    <tr>\n      <td>4820</td>\n      <td>8.154400</td>\n    </tr>\n    <tr>\n      <td>4830</td>\n      <td>8.136300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model saved to './fine_tuned_blip_vqa_lora'\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport zipfile\nfrom IPython.display import FileLink\n\n# Define the directories to include in the zip file\noutput_dirs = ['./fine_tuned_blip_vqa_lora', './results', './logs']\n\n# Define the output zip file name\nzip_filename = 'fine_tuning_outputs.zip'\n\n# Create a zip file\nwith zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for output_dir in output_dirs:\n        if os.path.exists(output_dir):\n            for root, _, files in os.walk(output_dir):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    # Add file to zip with a relative path\n                    zipf.write(file_path, os.path.relpath(file_path, '.'))\n            print(f\"Added {output_dir} to {zip_filename}\")\n        else:\n            print(f\"Directory {output_dir} does not exist, skipping\")\n\n# Generate a download link for the zip file\ndisplay(FileLink(zip_filename))\n\n# Print instructions for downloading\nprint(f\"Click the link above to download '{zip_filename}'.\")\nprint(\"If the link doesn't work, go to the 'Output' tab in Kaggle, locate the file, and download it manually.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T17:34:16.242385Z","iopub.execute_input":"2025-05-13T17:34:16.242692Z","iopub.status.idle":"2025-05-13T17:34:21.634059Z","shell.execute_reply.started":"2025-05-13T17:34:16.242669Z","shell.execute_reply":"2025-05-13T17:34:21.633420Z"}},"outputs":[{"name":"stdout","text":"Added ./fine_tuned_blip_vqa_lora to fine_tuning_outputs.zip\nAdded ./results to fine_tuning_outputs.zip\nDirectory ./logs does not exist, skipping\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/fine_tuning_outputs.zip","text/html":"<a href='fine_tuning_outputs.zip' target='_blank'>fine_tuning_outputs.zip</a><br>"},"metadata":{}},{"name":"stdout","text":"Click the link above to download 'fine_tuning_outputs.zip'.\nIf the link doesn't work, go to the 'Output' tab in Kaggle, locate the file, and download it manually.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}