# Visual-Question-Answering
This Project involves creating a multiple-choice Visual Question Answering (VQA) dataset using the Amazon Berkeley Objects (ABO) dataset, evaluating baseline models, fine-tuning using Low-Rank Adaptation (LoRA), and assessing performance using standard metrics. 

How to run the code:
Using the requirements.txt file, create a new environment (Python 3.9) and install the dependencies mentioned in the file.
Run the inference.py file using the following command:
python inference.py --image_dir <PATH-TO-IMAGE-DIR>  --csv_path <PATH-TO-IMAGE_METADATA-CSV>

Please refer to the report.pdf file for more details about the models, files, steps followed and the final results of the project. 
